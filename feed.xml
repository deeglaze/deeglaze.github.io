<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://deeglaze.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://deeglaze.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-04T17:17:40+00:00</updated><id>https://deeglaze.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal homepage for computer scientist and software engineer Dionna Glaze, Ph.D. </subtitle><entry><title type="html">Transparency has value</title><link href="https://deeglaze.github.io/blog/2025/Transparency-has-value/" rel="alternate" type="text/html" title="Transparency has value"/><published>2025-06-19T00:15:00+00:00</published><updated>2025-06-19T00:15:00+00:00</updated><id>https://deeglaze.github.io/blog/2025/Transparency-has-value</id><content type="html" xml:base="https://deeglaze.github.io/blog/2025/Transparency-has-value/"><![CDATA[<p>In this post, I want to highlight where the value is for hyperscalers, and specifically public Cloud Service Providers (CSPs), to reach for transparency standards and surpass them when it comes to being in their customer’s trusted computing base.</p> <h1 id="trustworthy-computation">Trustworthy computation</h1> <p>Confidential Computing has a promising potential that has not yet been fully realized in CSPs like Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP). The general idea is that by using this technology, you are assured by the hardware manufacturer themselves that</p> <ol> <li>you <strong>can</strong> know exactly what the service you’re talking to will do with the data you give it (integrity reporting with remote attestation)</li> <li>the host platform will have limited visibility into the computation, only what the service elects to share (confidentiality through memory encryption)</li> <li>the host platform will have limited influence on the computation, since it can only offer information that the service can elects to act upon or not (integrity enforcement)</li> </ol> <p>There are nuances in how the different technologies ensure those properties in order for their claims to be trustworthy. It’s important for the platform maintainers to promptly roll out security updates as flaws are found and fixed with microcode or firmware updates. The platform firmware version numbers are part of the remote attestation report so you can keep track of your platform provider’s obligation to keep hosts maintained.</p> <p>In my previous post on standards, I gave a realistic look at what it means to build trust in the trusted computing base that is measured for remote attestation.</p> <h1 id="opacity-will-not-be-tolerated">Opacity will not be tolerated.</h1> <p>All CSPs right now introduce their own virtual firmware into confidential VMs. We know from our own sensibilities and many customer conversations that binary blobs simply aren’t enough. Folks are willing to accept incremental progress from nothing, to blob, to signed blob, to yet more information, but we know at least one thing: CSPs MUST allow customer-provided virtual firmware (with a well-documented interface for achieving UEFI variable persistence and ACPI table information) OR publish the sources for their virtual firmware. Doing both is better, but given the amount of management folks are coming to the Cloud for, published sources are a higher priority.</p> <p>In the ACM article, <a href="https://queue.acm.org/detail.cfm?id=3623460">Why should I trust your code?</a>, the authors describe a Code Transparency Service that non-repudiably logs claims about the binaries that ultimately measured and included in remote attestation reports. The publication’s research service as described is remarkably similar to https://sigstore.dev for tracking in-toto attestations. Let’s concretize with in-toto.</p> <p>What claims are foundational to really understanding the software? At the bare minimum, the binary should be available such that its measurement can be extracted from it, and potentially analyzed, but that is unsatisfying to those of us that don’t spend their days staring at disassembly. You <strong>can</strong> know what it does, but it’s time-consuming and error-prone.</p> <p>Let’s say beyond the actual bits, you need assurance that the binary was released by a particular party. Sigstore tracks a signature of the binary for a claim of authenticity. You <strong>can</strong> know that the binary does what it does if you trust what the party says about it.</p> <p>Customers are saying that they don’t trust CSPs at their word. Firmware is too complicated for a single organization to get it right. They need to see it, or they need to have trusted parties see it and give their endorsement.</p> <h1 id="reproducibility-source-is-not-enough">Reproducibility: source is not enough</h1> <p>If I provide a repository of source that has a Makefile, and I say commit XYZ corresponds to binary B, but when I run <code class="language-plaintext highlighter-rouge">make</code> on my machine, the two binaries are different, then how can I trust that claim? Do I really have to rebuild everything from scratch to believe this claim?</p> <p>If you can maintain a reproducible build, that’s fantastic. Anyone who trusts their own build environment can sign a claim that they were able to reproduce the measurement from the sources to not need to do a full build on the fly during an online attestation verification.</p> <p>Indeed AWS has a reproducible UEFI that corresponds to their attested measurements (https://github.com/aws/uefi). Signal messenger has a reproducible build for their contact discovery service with their releases’ MRENCLAVE value represented in the branch name (https://github.com/signalapp/ContactDiscoveryService-Icelake). Project Oak has a reproducible build of their stage0 firmware (https://github.com/project-oak/oak/blob/main/README.md#transparent-release).</p> <p>I don’t think reproducible builds are a particularly durable property to maintain over a project’s lifetime, especially if everything is expected to shift to confidential computing.</p> <h2 id="durable-properties">Durable properties</h2> <p>When toolchains and other dependencies are updated, non-determinism tends to creep in. Most starting points for common dependencies do not include reproducible builds. Not every project for libraries and packages used for confidential computing environments are fully committed to maintaining binary reproducibility of their build configurations.</p> <p>For a durable property, we need the build service’s claims linking inputs to outputs to be trustworthy even when the service operators aren’t fully trusted. The SLSA build environment track is working out the standards now, and it looks like the highest level of assurance will require the builds themselves to be in confidential computing environments, with attestation verification done by a trusted verifier against a policy that is also part of the build attestation.</p> <p>AWS has implemented and formally proven correct an attestation verification engine called <a href="https://docs.cedarpolicy.com/">Cedar</a>, so the margin of error for policy interpretation to build trust in a build is near zero. It’s not yet clear if Cedar can be used for verifying confidential computing remote attestations, but I would expect it is on their roadmap. The engine, and not the verification service, is verified, so you’d still have a bit of a gap to cross to build trust, but the margins are shrinking.</p> <p>What’s clear is that trust in the build service can eventually be reduced to the build environment definition and its attested runtime protections. Provide the source and the toolchain container, and you can trust the built outputs came from running that toolchain on those sources, regardless of reproducibility.</p> <h1 id="source-transparency">Source transparency</h1> <p>What the ACM article and Project Oak have in common is a focus on transparency. When sources are firmly linked to the measured binaries with verifiable (indeed falsifiable) tests, you can start building understanding. Since source code is written for humans, it’s this starting point that allow experts to audit published sources for potential problems.</p> <p>Let’s work from an example of some component you need that is running at version 1.0 with security version number 1. Say 1.0 is supplanted with 1.1 without a security version number increase. Without transparency, you wouldn’t necessarily know if this was adding something you needed and just updated to be at the latest version. Say the 1.1 release adds a feature you don’t depend on, but it also introduces a bug to a feature in 1.0. Auditors find the bug due to the commitment to transparency, and responsibly disclose the problem to the dependency in question. The dependency creators release a CVE along with a new release that fixes the bug, increase the project’s security version number, and say to update to 1.2 since the latest security version number is 2. You don’t really need to do all that if you’re doing fine on 1.0, since it’s not affected. You know you’re not affected because you too can read the code. You can wait to adopt 1.1, and will be glad you did because it’s flawed.</p> <p>It’s a bit strange to say that security version number 2 is “most secure”, but this is how versioning is. Software might be released in the single dimension of linear time, but the behavior of those releases don’t follow a single dimension. With in-toto’s <code class="language-plaintext highlighter-rouge">vulns</code> predicate, however, every artifact that’s within its service lifetime can should have “heartbeat” endorsements of what is known about that artifact. It’s a “heartbeat” since it should only last a short while, as “current” gets outdated quickly. You can learn that the dependency you’re on is not affected by any particular CVE. The 1.1 release might have a non-empty “known-CVEs” endorsement. It’s this granularity of nuance that claims and access to source code allow you to best understand your security posture.</p> <h1 id="release-transparency">Release transparency</h1> <p>Let’s now talk about CSPs’ ability to change your TCB without your awareness. The virtual firmware is still the CSP’s responsibility. In 2023 there was a UEFI vulnerability called BlackLotus. Machines running affected firmware had to be updated. Since the CSP is the firmware provider, they need to force the VM to restart into a mitigated firmware. This happens faster than most customers managing their own firmware would have been able to orchestrate on their own. Still, it’s troubling for confidential computing customers to know that at any time their VM could reboot on a different (still signed) firmware that attestation verification services might be updated to allow, but which is still unannounced.</p> <p>There was previously no reason to announce a new firmware version. The firmware should maintain its behavior and update transparently. But software does not progress linearly.</p> <p>Firmware release candidates should be announced with their sources and measurement well ahead of their rollout to allow for auditing and reports of flaws. The problem with this is there is no opt-out of a new release. If you manage your own verification service instead of using the CSP’s provided service, you will be hit with unknown measurements frequently when there is no forewarning. One of the leaders of the Confidential Computing Consortium, Mike Bursell, believes that maintaining an attestation verification service is a viable business as a means of putting in more effort in finding quality signal, so there’s certainly a reason to support third party verifiers.</p> <p>Unannounced releases remove customer agency and drives many to ask for the ability to bring their own firmware. Bringing your own firmware allows you to innovate faster with features you need, not just stick to tried and true versions without new unused features; consider Project Oak’s small and hardened stage0 firmware.</p> <h1 id="interface-transparency">Interface transparency</h1> <p>Every CSP seems to have their own virtual machine monitor with bespoke virtual devices to interact with the tightly-coupled virtual firmware. OpenVMM and OpenHCL (from Microsoft, now under the Confidential Computing Consortium) use the <code class="language-plaintext highlighter-rouge">firmware_uefi</code> virtual device. Qemu uses <code class="language-plaintext highlighter-rouge">uefi-vars</code> for UEFI variable storage and <code class="language-plaintext highlighter-rouge">QemuFwCfg</code> for configuration of things like ACPI tables. AWS uses <code class="language-plaintext highlighter-rouge">ExtVarStore</code> for its UEFI variable persistence (and perhaps <code class="language-plaintext highlighter-rouge">QemuFwCfg</code>, I haven’t dug too deeply). GCP uses <code class="language-plaintext highlighter-rouge">PvUefi</code> for UEFI variables and telemetry about boot progress and <code class="language-plaintext highlighter-rouge">QemuFwCfg</code>, though the source is not public (all the diffs are buried under years of merges, so it’s hard to release cleanly without a good chunk of effort).</p> <p>The <code class="language-plaintext highlighter-rouge">firmware_uefi</code> and <code class="language-plaintext highlighter-rouge">ExtVarStore</code> devices are admittedly bespoke since there isn’t really a good gathering place for VMM implementers to decide the best way to provide a uniform experience with virtual firmware across platforms. It wasn’t really an issue before. But now we ought to come together to decide what is it that ought to be provided on virtual platforms so confidential computing use cases can be served everywhere.</p> <p>When the interface between VMM and user-provided firmware is clearly documented, and there’s a place for implementers and customers to come together and agree on what should be available, it’s not a bad idea to allow the user to bring their own firmware.</p> <h2 id="user-provided-firmware">User-provided firmware</h2> <p>The firmware-unified kernel image (<a href="https://people.redhat.com/~anisinha/fuki-ref.pdf">FUKI</a>) idea from AWS and RedHat allows a user provided firmware to replace the cloud’s firmware, as fetched from the disk image.</p> <p>The interface is not developed to a mature state for broad adoption–it is a subset of the firmware launch description capability possible with the IGVM format. The requirement for using the interface is not particularly popular–if we can ensure that the firmware is at a known location on the disk image, can we allow the CSP to launch the user firmware directly without the extra discarded boot sequence? We will need to solve these problems for users to have full control of their TCB and for CSPs to provide the best possible experience for their customers.</p> <h1 id="conclusion">Conclusion</h1> <p>There are many aspects of transparency, each with their own merits and their own value that confidential computing customers are aware of and have a desire for. Without commitment to transparency and a clear timeline to achieve it, I have strong reason to believe that will drive the most risk-averse and security-savvy customers to other platforms that take it more seriously.</p>]]></content><author><name></name></author><category term="attestation"/><category term="business"/><summary type="html"><![CDATA[In this post, I want to highlight where the value is for hyperscalers, and specifically public Cloud Service Providers (CSPs), to reach for transparency standards and surpass them when it comes to being in their customer’s trusted computing base.]]></summary></entry><entry><title type="html">Move fast, make shit</title><link href="https://deeglaze.github.io/blog/2025/Standards/" rel="alternate" type="text/html" title="Move fast, make shit"/><published>2025-03-01T16:15:00+00:00</published><updated>2025-03-01T16:15:00+00:00</updated><id>https://deeglaze.github.io/blog/2025/Standards</id><content type="html" xml:base="https://deeglaze.github.io/blog/2025/Standards/"><![CDATA[<h1 id="innovate-privately-on-trusted-technologies-and-weep">Innovate privately on trusted technologies and weep</h1> <p>I work on Confidential Computing at Google, and I’m idealistic when it comes to collaboration, developing in the open, and the construction of standards.</p> <p>Confidential Computing is in its infancy because it’s been conflating a few different ideas and has failed to get the ball rolling for some time now. The first idea is execution environment isolation through hardware-enabled cryptographic protections. The second idea is remotely attesting to the isolated workload’s code identity with manufacturer-endorsed claims that “this is the code you’re talking to (in the expected isolation environment)”.</p> <p>Everything in this post is in the context of enterprise software security, because if you care about software supply chain as a startup and aren’t a software supply chain startup… I mean, good on you, but this is a lot of cost and worry for smaller operations.</p> <h2 id="execution-environment-isolation">Execution environment isolation</h2> <p>Enabling both the isolation technologies and remote attestation at a system level are years-long efforts even for Google and Microsoft. I was 2 years out of my Ph.D. before I started working on making SGX semi-usable, and we unknowingly wrote our own POSIX(-ish) operating system, Asylo. We had to simulate every system call and try to do so while being extremely skeptical of any information the untrusted environment presented. It was porous. After a few years, we hired the security researcher that sustained himself for a couple years off bug bounties on Asylo and froze development.</p> <p>In Intel SGX, AMD SEV, Intel TDX, ARM CCA, and RISC-V CoVE, all the execution contexts are at least protected through memory encryption. Intel SGX was the first to get enabled on Linux upstream but has failed as a technology for myriad reasons: vulnerabilities, limited hardware support (90MiB RAM total???), and a difficult programming model are all valid “final nails” to seal its coffin.</p> <p>With AMD SEV, we see the execution isolation capabilities scaling from small “enclave” solutions to full virtual machines. A VM can be “enclave-like” with specialized virtual firmware, but to get there practically we have to start where people are <em>now</em> with their big environments. Project Oak has their stage0 firmware that launches a WASM environment. It’s the closest to an SGX on SEV-SNP situation as I’ve seen. I will have a hard time making it available to use on Compute Engine though, and that’s another story. AMD SEV had a way to measure its code at launch, but the workflow to do so was not easy for cloud platforms to enable, and it wasn’t real remote attestation with PKI.</p> <p>I think with AMD SEV-SNP and Intel TDX host patches landing in upstream Linux, things are finally starting to get interesting. Apple launched their private AI based on SEV-SNP, Microsoft launched Confidential Inference, Google’s got Confidential Match for ads privacy, and that’s just the beginning.</p> <p>Google, Microsoft, Oracle, Alibaba, Intel, AMD, IBM, Huawei and yet more organizations’ emails are all represented on the Linux Kernel Mailing List patch review threads over the years to get these technologies supported in Linux. The addition of virtualization technologies that promise to remove the hypervisor from the trusted computing base (TCB) add a fundamentally different threat model to the Linux kernel. In fact, bug fixes for closing holes related to that threat model would not be accepted for many months because we didn’t have an agreed-upon baseline definition for what the threat model was. Thankfully, Elena Reshetova (Intel) and Carlos Bilbao (AMD, then) authored and got merged such a <a href="https://www.kernel.org/doc/Documentation/security/snp-tdx-threat-model.rst">baseline threat model</a>.</p> <p>A single threat model for multiple technologies. We should expect the AMD CCA and RISC-V CoVE patches once they come to further add themselves to this threat model document. We converge on founding principles.</p> <h2 id="remote-attestation">Remote attestation</h2> <p>Before I talk about the potential good of remote (software) attestation, I must first address the elephant in the room.</p> <h3 id="software-attestation-as-a-big-bad">Software attestation as a “big bad”</h3> <p><a href="https://datatracker.ietf.org/doc/statement-iab-statement-on-the-risks-of-attestation-of-software-and-hardware-on-the-open-internet/">Software attestation is a term soaked in controversy</a>. At its core, cryptographic signing of exact software bits is usable for making claims of authenticity. “This code is known to be exactly the same as (those produced by corporation A)” is a valuable statement for protecting enterprise workloads from insider threats on running systems. “This code is known to be exactly the same as (MPAA-allowlisted video decoders that are given access to decrypt a video purchase)” is technically the same form of statement but much more chilling to a consumer or art historian. End User License Agreements (EULA) attached to physical and virtual things we buy are long legalese that boils down to, “you don’t own this. You’re buying limited access. Also you can’t sue us in court because we figured out people will just accept that now hahaha.”</p> <p>Remote attestation, cryptography, and encrypted execution environments make it possible to protect cryptographic keys in use, keep them stored remotely in a secure location, and only release the keys to authorized software. This is a powerful capability is used to protect your Chrome passwords, access to your personal data for personalized ads, and soon so much more. On the client side though, we’re looking at the same forced network connectivity to access your purchased content the way Valve’s Steam pushed on us over a decade ago. If a key server is down, your ISP has an outage, or you’re in most any rural area, you are out of luck. The digital divide grows wider with added restrictions and meager developments in widespread free or low-cost internet access.</p> <p>Cory Doctorow came to speak at Google years ago to decry DRM technologies that lock consumers out of their own hardware. His story was similar. A colleague of mine decided to troll him, though. “I <em>love</em> DRM technologies, because they can better protect customer privacy in the Cloud!” Not all the way wrong, but Cory was not on his back foot.</p> <p>If SGX taught us anything, it’s that hubris is folly. Cryptographic isolation technologies will never be bullet-proof.</p> <blockquote> <p>All it takes is one machine to have its fused secrets extracted for the system to tumble down. The Cloud provider still has control of job scheduling. If you have a target, you just send them to the compromised machine and then extract their secrets.</p> </blockquote> <p>A not-incorrect quip. A very real situation in fact, given <a href="https://github.com/google/security-research/security/advisories/GHSA-4xq7-4mgh-gp6w">Google did indeed find a way to steal a chip’s secrets</a>. It does speak to the increased cost of an attack, but it doesn’t stop a motivated, skilled, and well-funded attacker.</p> <h3 id="untrusted-hosts">Untrusted hosts</h3> <p>The “selling point” of Confidential Computing is that it removes the host from the trust boundary, or at least makes boundary violations more discoverable through remote attestation. This is mistinterpreted by many to say, “the host is untrustworthy and if they have any kind of security hole, then they will immediately attack it.” You have to trust your host service, because there is no unbreakable system, especially with physical access. It makes <em>no</em> business sense to attack your customers, unless compelled at gunpoint like my colleagues in Russia were.</p> <p>Confidential Computing is a means of stopping your trusted hosts from being able to comply with blind subpoenas (or other legally questionable data breaches). So, how do we make compelled attacks detectable?</p> <ul> <li>We make it really hard to execute (bad word choice) by closing as many unchecked communication channels as we can.</li> <li>We sign our software to make one-off builds with malware stand out.</li> <li>If a compelled backdoor is forced through the proper channels to roll out to everyone, a good host will try to telegraph its operations: <ul> <li>Make the user experience terrible so the change is noticeable to customer performance metrics. If the compelling force is not so callous as to destroy the entire business and perhaps instigate public outrage, they will back off and allow business to restore non-malware to the fleet.</li> <li>Have a business practice of posting signed binaries at least a couple weeks before they’re rolled out to allow for researcher so do binary differential analysis. A patient compelling force may still be fine with this, since it’s expensive to reverse-engineer binaries.</li> <li>Have a business practice of posting the source code that correspond to its binaries to allow for auditing. Compelling force in a non-authoritarian jurisdiction is not permitted to compel publishing false statements, so malware could be caught by audit. There are still legally required to <em>not</em> post source code, in the case of vulnerability embargo. Embargos are contractual agreements to delay publishing the fix or the vulnerability details ahead of a contractually agreed-upon date as condition to being made aware of the vulnerability and/or its fix.</li> </ul> </li> </ul> <p>Now, this all assumes that the host is the one in charge of the TCB like the virtual firmware. People pay for hosting because they want the maintenance operations taken care of for them, so it makes sense that something as extremely host-specific as virtual firmware will be provided by the host.</p> <h3 id="virtual-firmware">Virtual firmware</h3> <p>Every Cloud Service Provider (CSP) has their own virtual firmware they put in customers’ VMs, unless the customer rents an entire server blade to manage the “bare metal”. Azure previously had a private preview for folks to bring their own firmware, but they deleted the blog post about it to memory-hole the effort. Google controls its virtual firmware. AWS controls its virtual firmware, although it is published online with a reproducible build (based on a 2022 EDK2 release, so it’s not well-maintained).</p> <p>All of the compelled firmware takeovers can be very discoverable with Bring Your Own Firmware. If the measurement is not a measurement you made yourself and trust because it’s from you, then don’t release secrets to that machine.</p> <p>Azure tried this though, and not enough customers wanted to maintain their own firmware. There isn’t really an easy way to debug virtual firmware when you don’t get to see how the VMM works. We don’t have a consortium of hypervisor technologies where we try to offer a consistent experience to folks across platforms. Google’s Vanadium uses KVM and has some Qemu device emulation capabilities, but it’s still not Qemu. Hyper-V is another world. OverVMM was donated to the Confidential Computing Consortium (CCC), but it will not be what any other major CSP starts to offer folks.</p> <p>We need trusted second parties to figure out how to provide trusted firmware, and sell that maintenance cost in order to diffuse the responsibility away from the host. RedHat, a long time maintainer of Qemu and OVMF (a flavor of EDK2), is a great example of a party that folks ought to consider trusting to provide virtual firmware maintenance licenses. CSPs have to build support for it though, and it’ll cost tens of millions of dollars realistically. There’s not enough customer demand.</p> <p>I sure do wish we could standardize on something. Microsoft is doing a lot of open source publishing with OpenVMM, OpenHCL, OpenEnclave, contributions to Coconut-SVSM, and the IGVM format. Whereas my employer and Microsoft are battling over GPUs and AI, I think there’s a lot of decent professionals that could make the VM ecosystem better for their customers by talking to each other. The CCC is one place that’s happening, but it doesn’t have the charter for standardizing hypervisors. The more standards, the more redundancies we can eliminate.</p> <p>However, the more standards, the more commoditized a platform becomes (as seen by business folks). For something as small as how to launch a VM and operate a UEFI, I don’t think there’s a lot to differentiate, so there’s not a lot of business opportunity in avoiding open collaboration.</p> <h3 id="transparency">Transparency?</h3> <p>Let’s say I require absolutely all the IP in my CSP’s host stack to be publicly available, built with a toolchain I trust, and reproducible down to the bits that I see in remote attestation. You better be a government spending billions and be happy with an NDA to view the source code, because no way any CSP will publish their infrastructure for competition to copy. Each CSP has spent billions of dollars to learn how to build that infrastructure, and they want their competition to have that same opportunity.</p> <p>So let’s just say virtual firmware. Confidential Computing means you don’t have to trust the host, right? Wrong, but trust less I suppose. Still, virtual firmware is mostly all publicly available in EDK2 because of the OVMF project. There is some glue to paravirtually communicate with the host VMM because emulating SMM (x86-specific) for flash storage is foolish from a security perspective. There may be some different measurements to TPM PCRs than upstream.</p> <p>This is not a big cost to make public, right? Embargos make this hard. Say you’re a UEFI forum member (~$3,200/yr) and have access to the security group communication. You might negotiate access to the repository to stay within the embargo. Okay, so which toolchain should the CSP use to build those sources?</p> <h4 id="toolchain-trust-is-the-hardest">Toolchain trust is the hardest</h4> <p>Google at least has a toolchain team that contributes fixes to LLVM and other tools, and keeps an internal release close to HEAD. There is no publicly available Linux distribution that has that cadence of developer tool package deployments. So, Google trusts that toolchain. It may even have embargoed patches in it, I don’t know. It certainly did during the Spectre/Meltdown situation. So, Google trusts firmware built by its toolchain. Google does use Google Compute Engine, so if we just have the one firmware, we’re going to want “max good” for ourselves.</p> <p>Do you trust that toolchain? You probably should, since a compelled attack on a customer through first compromising the compiler to insert malware into EDK2 is really hard to do. If the toolchain container gets published (just the binaries), then a compelled software supply chain attack would be much more obvious with a binary differential analysis.</p> <p>If instead you’re more of a purist and want a non-Google toolchain to build the firmware, then I don’t know why you would trust Google to produce the maintained EDK2 source code either. Say you do, though, for some reason. Say we build two firmwares: one with our maintained toolchain and one with.. I dunno, RedHat’s? No, their sources are behind a paywall because you ought to pay for maintenance. Let’s say Debian. They’re a principled bunch. Oh, that’s a really old compiler package. Hope you don’t have toolchain problems that need to be fixed, which often happens with the CLANGDWARF build of OVMF.</p> <p>Okay. Toolchain is built transparently. OVMF is built transparently and somehow works. We deploy that as an option. But you get the deployed firmware since it’s tighly coupled to the VMM version. One reset and you upgrade. Are you on a transparent release? Oh, okay good. Did you audit the changes in time? Oh, okay good. Oh there’s a problem with it? That’s going to take a lot longer to fix, and won’t be prioritized until lots of customers decide they want the worse-maintained firmware.</p> <p>I don’t know how to please everyone with just one or two firmwares. I would point you back at Bring Your Own as a demand for your host. The whole firmware distribution and compatibility situation has to change, which I repeat I would judge as costing at least 20 million dollars to build, and at least a million yearly to maintain.</p> <p>You should still absolutely demand as much transparency as is reasonable. Maintenance costs, and you’re paying for it by using the platform anyway. But you can at least get sources and a build attestation that the sources correspond to the published binary using Google’s maintained toolchain. With Bring Your Own, you get to choose when you upgrade. You can read our latest updates and then use your agency to bring the most recent release to your VM.</p> <h2 id="your-tcb-will-never-be-small-enough-but-it-should-be-inventoried">Your TCB will never be small enough, but it should be inventoried</h2> <p>I don’t know about you, but I have a hard time trusting myself to get things right all the time. I do know that I can trust myself to not intentionally backdoor a system. Now imagine a system built by several companies through open source collaboration and private innovation. Can you trust all of them? What about who they trust? Do you even know who that is?</p> <p>You’ll have to, realistically, to get work done, but there is something you can start to do to improve the trustworthiness of code generally. Go to your projects and ensure you have good enough source controls enabled, where “good enough” is scored by the OpenSSF scorecard. Do it for your own projects first before you try to encourage any dependency projects to do it, because you need to understand what it is you’re asking people to do. If your dependencies do not have the time to do OpenSSF scorecard, give them grace, and see what your employer can do to support its maintenance. If your dependency cannot be maintained sustainably, then fork it and maintain it yourself. If you can’t do that, then you’ve identified a weakness in your supply chain that ought to be documented as a risk to your project’s health.</p> <p>Now imagine that we live in a world where your transitive dependencies have demonstrated that they’re well-protected from supply chain attacks: no unilateral changes, no downloaded dependencies without integrity-checks, and even demonstrated responsiveness to vulnerability reports. Let’s say they’re all scoring 90%, an A- on a difficult comprehensive test. Your dependencies show good signs of health, but these are still metrics that can be gamed.</p> <h3 id="a-plea-for-developers-and-companies">A plea for developers and companies</h3> <p>If you maintain an open source project, please don’t sell yourself short. Make your responsiveness to issues a paid service. Make your dependency maintenance and release cadence a paid service (if the code is in maintenance mode, update your toolchain and enable the latest sanitizers and warnings).</p> <p>The open source spirit is essential to collaboration to solve big problems that are shared across companies. The open source business model is the tragedy of the commons, and the plundered, unfunded maintainers can be leveraged against your company. Please pay maintainers for service level agreements (SLAs). We need more contractual agreements with financial support to reduce risk to supply chain health and to combat the tragedy of the commons.</p> <h3 id="attestation-verification-services">Attestation Verification Services</h3> <p>The internet runs on Transport Layer Security (TLS) for websites to use HTTPS. The security of the internet comes from the Certificate Authorities (CAs) that are paid to do the due diligence that a certificate signing request comes from someone who can demonstrate their ownership of a domain. The internet depends on domain owners to keep their keys safe. This ensures that communications between you and that domain are encrypted by keys only you and that domain operator have access to.</p> <p>Notice that none of this trust is based on what that domain operator does with any of your communications to them. If you’re communicating sensitive information, like in a patient portal on an online banking service, then your trust is in the name alone that they are doing what they say they’re doing with your data.</p> <p>The HIPAA, GDPR, and PCI/DSS regulations have some bearing on those operations, but security standards don’t have supply chain protections. They don’t have requirements for certain code quality metrics.</p> <p>Let’s be honest about Remote Attestation letting you know exactly what software you’re talking to. You can have a trusted firmware, a trusted cloud provider, a trusted bootloader, operating system, initrd, discoverable disk images, one of which contains some kind of workload orchestration daemon and another that handles the credential activation. Did you audit all of those? Read all 10 billion lines of code? Of course not. It’s the producers of that code that you have to trust, ultimately. How do we account for all the folks that produced the code that the domain operator is running? This is the role of an Attestation Verification Service (AVS).</p> <p>An AVS like Google Cloud Attestation (GCA), Microsoft Azure Attestation (MAA), Intel’s Trust Authority (ITA), or SPIRE takes in a bunch of evidence, verifies all the signatures, does… something.. with the measurements, and presents the Attestation Results in some kind of signed token. What is that something? What are the results? How can a lay person make sense of the results?</p> <p>The answer is very nuanced, because right now everyone is trying to produce something, anything, that satisfies some of their customers’ requirements. Requirements in this are not particularly well-scoped. So we all do something different.</p> <p>Intel Trust Authority has grand visions like a lot of us do, but we need to build a lot more infrastructure and convince people it’s worth using before any AVS is truly successful. At the moment can just verify a TDX quote as genuine and was generated by a host with TDX TCB with a particular security posture (e.g., some TDX module releases have had security bugs). What else it quoted, like the MRTD and RTMRs and other measurements (MR is “measurement register”) are unchecked, but… could be checked. MAA similarly checks the authenticity of the quotes, but just parrots back the measurement values it thinks might be important to you; they could be checked by MAA, but they’re left to the relying party to check for themselves using an source of reference values. They likely are working on more features, but I’m unfamiliar with them beyond the documentation.</p> <p>GCA works for SEV VMs, so it trustes the host, but it at least as a host-provided virtual Trusted Platfor Module (vTPM) that measures all the boot components and signs them with a Google-issued key; it checks Confidential Space measurements. Confidential Space is a product that runs a special flavor of Container-Optimized OS (COS) called Attested COS, or aCOS. aCOS emphasizes measured-boot integrity protections and a more locked-down user space, which includes a trusted container launcher that will also measure the digest of the one container it’s allowed to launch. So, GCA checks that secure boot passed muster and that GRUB2 measured the kernel and (integrity-protected) initrd that were built for that version of aCOS, and it produces a token that the container digest is indeed running on the Confidential Space platform. Knowing that a single container digest is running on your platform is still not particularly useful for relying parties. They get a very restricted policy language called Common Expression Language (CEL) that’s basically propositional logic with basic comparisons in order to assign a “Workload Identity Pool” to a matching Attestation Result. It’s inclusion in this pool that can be used in Identity and Access Management policies to restrict access to certain resources only to acceptable workloads running in Confidential Space.</p> <p>When you have vertical integration of firmware down to user space, it’s much easier to know what reference measurements to use, but if you want a “general purpose” AVS, there’s so much more to do. Let’s talk about what a coherent ecosystem for general purpose AVSes might look like.</p> <h3 id="attestation-ecosystem">Attestation Ecosystem</h3> <p>Let’s say you want to may your own flavor of Confidential Space, but you want to use some form of general purpose Attestation Verification Service. What would that look like? The only company that has written down enough information about what this would look like soup to nuts is ARM, since they sell their IP for other folks to implement, and it better have clear documentation. They have a reference AVS implementation at https://github.com/veraison/services. All the inputs and outputs of the Veraison stack are documented in Internet Drafts on the IETF datatracker, and are going through the RFC process. By doing this all out in the open, they’re inviting other folks to join the conversation and to ensure we can all satify each others’ needs with a small set of (extensible) standards.</p> <p>The generic formats they’ve been specifying include EAT, CoRIM, and CMW. There are specific uses of each of these formats that they use for ARM CCA and Veraison.</p> <p>ARM CCA uses the Trusted Computing Group’s DICE measured boot standard to measure the boot chain up to the Realm Management Monitor (RMM). The RMM itself has a Platform Service Attestation Token with the DICE boot chain’s verified measurements in it to certify the key that signs CCA tokens for each of the confidential VMs (realms) the RMM launches. These tokens are similar to TDX quotes and SEV-SNP Attestation reports, except it splits the platform from the guest. Where TDQUOTE has MRSEAM and CPUSVN and SEV-SNP has all the TCB_VERSION entries that each describe host TCB state, ARM puts those concerns into the separate PSA token.</p> <p>To understand the values of these tokens, there’s a need for signed reference measurements that are understandable by all the AVSs that want to support ARM CCA. These are CCA endorsements, and they use CoRIM to wrap “reference triples” to match target environments with reference measurements that will appear in the CCA tokens. Endorsements do not have to cover every measured element. Just as there can be a different firmware provider from bootloader provider, so too can there be different endorsement providers for their respective components. Signed endorsements can be published for each component, ingested by AVSs, matched against evidence, and then the Attestation Result can carry information about “who” signed “what”, or simply fail if an additional policy evaluates these whosiwotsits as unacceptable.</p> <p>Is this still too much information? You bet your butt it is. If we’re talking about a global ecosystem of everybody signing whatever they want and making it available to match against for attestation results, then that’s an abuse vector. So we use federation: only take information about particular feeds of software from particular sources of endorsements. But if we’re using pub/sub and staying up to date, then we have to be careful about what view of the endorsement database we’re presenting to the AVS so the information is internally consistent. We also might have a problem with query latency if we need to query an endorsement database multiple times or with a complex query in order to gather all the revelant endorsements about the evidence being appraised.</p> <p>If you’re an AVS, now you’ve got an issue of which endorsement database is important to which customer and how to make the service scale. As an attester, you’re in control of what information you share with any particular party. As a user of an attestation verification service, you’re in control of which policy you want executed for a particular set of evidence. As a relying party, there’s a particular policy and set of results you’re looking for in the attestation results. It’s the policy selection that plays an important part. The policy decides which slice of the global federation of endorsement databases is relevant. The policy decides which claims to share with relying parties. How do we define these policies so they scale to the internet?</p> <p>Will anyone write policy that isn’t “every measurement has a signature, and that signature is from the set of binary providers I trust”? Any kind of online analysis of trustworthiness of a measurement is going to fall over at scale. That has to be done offline as batch jobs. So if you do analysis and find something concerning, there’s a good chance it’s a false positive, since static analysis is still pretty awful. Example from GitHub’s dependabot, paraphrased, “your little tool uses a language that has a vulnerability in its standard library for servers” and you’ll say, “yeah but it’s not a server” and move on. Rinse and repeat for a gajillion packages in your deployment. Still there could be something it finds that is indeed a problem, so you should have these scans on to check periodically, but they shouldn’t stop your endorsement pipeline. If the quality of these tools does matter to you sometimes, you can add the vulns in-toto predicate to your build attestation and surface that in attestation results. So, sometimes you might see a policy that says “every measurement has a signature, and that signature is from the set of binary providers I trust. Also <code class="language-plaintext highlighter-rouge">sensitive-dependency</code> better have a vuln score less than 7”.</p> <p>We’re just looking at signatures of binaries, so are we just doing secure boot again? No, because each and every deployment isn’t expected to be updated with new secure boot variables to retract binaries that have a score one user doesn’t like and another is fine with. At the end of this exercise, we have an extra layer of protection at the AVS after secure boot might let some bad measurements through. Once you’ve booted to user space, in a production system, you ought to not be measuring anything else into boot integrity registers. If you do, you do it to poison your environment in response to some intrusion detection or emergent human SSH access (after shutting down the workload and deleting the attested credentials).</p> <p>So for your own Confidential Space, let’s say you have an offline policy generator that synthesizes information from a set of endorsements into a single executable policy. A general AVS will not permit arbitrary computation. There will be limits to what you can express, and there may be billing consideration for the execution time charged to the execution of your policy. You may be able to get a WASM module to run on a representation of partially appraised and annotated evidence and be expected to produce some JSON token of claims, but that makes the data model of annotation a commitment from the AVS. You may more likely get an Open Policy Agent interface to run a Repo policy document, where you can query what is known about the evidence using provided functions. You could be looking at endorsed measurement registers as signed by different parties, and you decide who is expected to sign what, given your set of Trust Anchors (see the CoTS internet draft from IETF RATS). You could also be looking at a list of events that were replayed and integrity-checked against the RTMRs. Event log analysis is tricky using Rego, given the state machine involved in checking the expected invariants, and the sheer number of queries to the endorsement database you might need to make of event endorsements. Achieving any kind of acceptable means of understanding reference endoresments from a global ecosystem means getting involved in standards. At the end of all the checking, what do you report? Well… what is expected from the relying party?</p> <p>Relying parties have different levels of sophistication of what they want checked and reported to them. There is no one size fits all. If you’re using your bank’s website, you may just care that the server uses remote attestation in a trusted computed environment, and all the software is accounted for by the bank’s production governance policies. If you’re using private AI, you may care about just about all that information, and maybe even the identity of the binaries involved in feeding your data to models, with the same kind of transparency problem as above, but at least here it’s AI, so… AI AI AI attestation transparency AI AI.</p> <p>Then you have the workload attestations and the web of trust woven by microservices and service meshes. Do you care about the topology? Probably not, but this is a trust but verify situation. Best case scenario, the vendor goes through the accounting process to make it reportable, and in so doing finds a bunch of open holes and closes them. More likely scenario, they do the checkbox security to put pretty paint on their pumice stone of a service.</p> <h2 id="wrapping-up">Wrapping up</h2> <p>There’s more to talk about with respect to meaningful attestation results, automated policy construction, endorsement database federation, and a notion of governance oversight with respect to attestation policies. One thing is clear to me: attestation will continue to confuse everyone until we build things together to realize a standard. But, to “differentiate ourselves”, we’ll move fast, think some, and make absolutely unusable shit. For a topic as rich and frought as attestation and cryptographic protocols, we need to involve more expert minds. I may be an expert, but I work too much and need more folks. Did I mention this could apply to AI? AI AI woo get hype.</p> <p>Maybe this rambling will make more sense when I present it at OC3.</p> <p>Attestation, code integrity, and binary transparency has been my focus for 5 of my 8 years at Google. The first 3 were making the toolchain for Asylo, publishing it as open source, and making its website. Which was a learning experience. Fun fact, three other Asylo engineers still work on confidential computing with me at Google Cloud.</p>]]></content><author><name></name></author><category term="musings"/><category term="standards"/><summary type="html"><![CDATA[Innovate privately on trusted technologies and weep]]></summary></entry><entry><title type="html">Mentoring a SWE: Part 0</title><link href="https://deeglaze.github.io/blog/2024/Mentoring-a-SWE-Part-0/" rel="alternate" type="text/html" title="Mentoring a SWE: Part 0"/><published>2024-08-19T15:00:00+00:00</published><updated>2024-08-19T15:00:00+00:00</updated><id>https://deeglaze.github.io/blog/2024/Mentoring-a-SWE-Part-0</id><content type="html" xml:base="https://deeglaze.github.io/blog/2024/Mentoring-a-SWE-Part-0/"><![CDATA[<h1 id="mentoring-a-software-engineer-part-0">Mentoring a Software Engineer: Part 0</h1> <p>This is a series of posts I want to start as a way of thinking through how I approach mentoring. This may not follow the narrative structure of a journal, but they are notes to myself. I am not writing this as instructional material, but if you find value in it, do let me know.</p> <h2 id="goals">Goals</h2> <p>I find a mentor/mentee relationship to be effective when it is goal-oriented and time-bounded. When I mentor someone in a professional context, I have three goals, and they’re all about helping a mentee with their goals.</p> <p>Goal 1. Establish context. A mentee should be seeking a mentor to reach a goal. Am I a good fit to help them reach it?</p> <p>Goal 2. determine the areas that need growth in order to reach the goal. Why do they need help reaching their goal?</p> <p>Goal 3. nurture that growth with suggestions, encouragement, and check-ins.</p> <p>I have the explicit non-goal of presenting a rose-tinted view of myself from the mentee’s point of view. I don’t need to feed my ego with someone that looks up to me. I need my mentee to see that flaws and limitations are everpresent in the human experience, but we can still manage to improve upon ourselves.</p> <h2 id="goal-1-establish-context">Goal 1: establish context</h2> <p>Maybe someone wants help formulating <em>more specific</em> goals on their way to their main goal, say, a promotion. This is what I typically have people coming to me for. At Google there are plenty of resources about this. I need folks to dig a little deeper for a more intrinsic career goal.</p> <p>What I find to be fascinating to watch is how unprepared a person is when asked, “would you be happy in that role?” Certainly more money is motivator. The stress of working a role with more responsibilities that you can’t handle may tip the scales away from the cash bump.</p> <p>I need to determine someone’s motivations in order to effectively guide them. As a mentor, there’s no point in my suggesting someone do something they’re entirely unmotivated to do since they likely won’t do it. They won’t try and they won’t improve.</p> <p>Are these person’s motivations even scrutible to me? If so, I might be able to take them on as a mentee.</p> <h2 id="goal-2-identify-growth-areas">Goal 2: identify growth areas</h2> <p>This can take a fair amount of the mentorship time. I need to get to know someone and their concrete struggles to determine where they need help. A meeting with a mentee is typically only 30 minutes, so it’s important to try to drive the conversation away from being just a gripe session.</p> <p>Is this person an arrogant “misunderstood genius”? Is this person too hesitant about their own ideas to try to advance them? Is this person stuck in a rut and looking for a way out of a common grind?</p> <p>I can only do so much with our limited time together that I try to pick out just a couple aspects to work on, with concrete goals attached. If a mentee and I can agree on what to prioritize to work on, then we can get to work.</p> <h2 id="goal-3-nurture-growth">Goal 3: nurture growth</h2> <p>I will try to have my mentee internalize the three Hs:</p> <ul> <li>Be Humble.</li> <li>Be Helpful.</li> <li>Be Hungry.</li> </ul> <p>I find that staying humble will improve your relationships with people. People are generally more willing to hear you out when they like you.</p> <p>To be more than likeable, I think you need to contribute something more than comic relief–to be helpful. Being helpful doesn’t just mean picking up an on-call shift for someone going on vacation. Helpfulness can extend to all areas of the team and the business, depending on how you structure your help.</p> <p>If you’re “fat and happy”, you have little motivation to improve. Therefore, I find that keeping momentum in self improvement requires carrying an intrinsic amount of hunger to have a greater impact.</p> <h2 id="closing-time">Closing time</h2> <p>I plan to go into more detail about how I go about achieving my goals as a mentor in later posts. Next, I want to detail some goals folks have had (abstractly) and how I went about or would go about getting more information about them.</p>]]></content><author><name></name></author><category term="musings"/><category term="mentoring"/><summary type="html"><![CDATA[Mentoring a Software Engineer: Part 0]]></summary></entry><entry><title type="html">Blogging again</title><link href="https://deeglaze.github.io/blog/2024/blogging-again/" rel="alternate" type="text/html" title="Blogging again"/><published>2024-05-25T21:30:00+00:00</published><updated>2024-05-25T21:30:00+00:00</updated><id>https://deeglaze.github.io/blog/2024/blogging-again</id><content type="html" xml:base="https://deeglaze.github.io/blog/2024/blogging-again/"><![CDATA[<p>I’ve had a few blogs in my lifetime. College. Grad school. First time parent. Pretty much every one of them had math in them. Yes, even when writing to my infant in some hypothetical future when they could understand me. Now I’m starting back up with my “professional” slice of life, but still with a fair amount of philosophizing and experimental thinking. I just want to tell stories again in a longer form and in a manner I feel more ownership of.</p> <p>I need this post to be more than, ¨more soon!¨, so I’ll talk about why I feel the need to own this content more than what I post on social media. Ownership is the topic of this post. OooOooh, so adult. Don’t worry, I’m still financially illiterate intentionally because I ethically disagree with gaming the system of capital to enrich myself without adding any social good. Could I then turn around and reinvest any capital gains back into community? Yes, but that thinking is how we get to the system we have today, where communities that are systematically disenfranchised by the owning class are expected to tell sob stories for pity money that is a tiny fraction of what tax-based governmental support could otherwise provide. This is where I have to follow up with and also explain I’m not a monster and still do donate my money and time to community. Yes, that. The sob story comment comes from the frustated stories of a friend that is a former executive director for a reasonably large 501c(3) that provides services to victims of gender-based violence. This whole explanation is condemned by fascists as virtue signalling, and of course the truth is in neither extreme. I’ll probably have a post later about my understanding of “truth” but not in this post.</p> <p>Popping the stack to get back from the meta-post to the post. I currently work for Google, one of the powerhouses behind “free” web-based products, and this is of course a topic of consternation for me. As a broke student, free was good enough. I didn’t care much about longevity, just access. Getting the invite to the gmail beta was amazing. Getting the invite to “the facebook” was amazing. I did still worry about longetivy while signing a license agreement that I own nothing of what I just paid for. I still signed it. There aren’t a whole lot of people with enough privilege to reject the license agreements and still maintain their ability to carry on with life normally.</p> <p>I could go on about the inequities about access to art and history more generally, but honestly I just wanted to be able to share the things I loved with my child when they grew to an age where they could appreciate it. Children grow up in their own cultural context though. I didn’t care about my parents’ love for certain types of music and art. I complained during the museum trips. I have the math books that inspired me. The video games that defined me. The movies that shape how I process social situations with direct quotes as a means of forming a shared understanding. I’ve tried to share what I love, but I don’t have particularly inspiring energy, or a pedagogical skillset that could help convey why I love certain things. My kid gets to be their own person and reject anything I try to share as a means of connecting about something we could both love. I still do get upset at the thought that I couldn’t even have that chance to share a culturally meaningful artifact due to some servers shutting down or the legal nightmare emulator authoring can be to preserve old technologies. I’m not sure how dire it is though, of all things. I’m glad there are people that fight all the wall-building.</p> <p>In terms of owning my own words, I’m just at a point where I’m more comfortable writing human-readable text files I can easily port to other publication methods. I don’t have my twitter posts backed up. They don’t maintain the context of the conversations I was having. They’re not particularly great at conveying a complete thought that could be revisited years later the way articles can. The fall of Twitter is not as much of a blow to me as it has been to other communities, but mostly because I’ve been rather insular since the start. Folks I wanted to maintain a connection to also had the opportunity to experiment with Mastodon before the truly unhinged days arrived. Still, even with my participation in the Fediverse on an instance I financially contribute to, the medium is different from here, where “the feed” is all me, and longer.</p> <p>These words are published with no license. They are all under my copyright. I do not consent to their use in any algorithmic training dataset.</p>]]></content><author><name></name></author><category term="jekyll"/><category term="update"/><summary type="html"><![CDATA[I’ve had a few blogs in my lifetime. College. Grad school. First time parent. Pretty much every one of them had math in them. Yes, even when writing to my infant in some hypothetical future when they could understand me. Now I’m starting back up with my “professional” slice of life, but still with a fair amount of philosophizing and experimental thinking. I just want to tell stories again in a longer form and in a manner I feel more ownership of.]]></summary></entry></feed>